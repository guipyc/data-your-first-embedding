{"cells":[{"cell_type":"markdown","metadata":{"id":"gD8RiBhYB6Ij"},"source":["# Your first embedding\n","\n","## Exercise objectives:\n","- Run your first RNN for NLP\n","- Get a first taste of what an embedding is\n","\n","<hr>\n","\n","Words are not something you can easily feed to a Neural Network. For this reason, we have to convert them to something more meaningful. \n","\n","And this is exactly what _Embeddings_ are for! They map any word onto a vectorial representation (this a fancy way to represent each word with a vector ;) ). For instance, the word `dog` can be represented by the vector $(w_1, w_2, ..., w_n)$ in the embedding space, and we will learn the weights $(w_k)_k$.\n","\n","So let's just do it.\n","\n","\n","# The data\n","\n","\n","❓ **Question** ❓ Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n","\n","⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n","\n","⚠️ **DISCLAIMER** ⚠️ **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy. "]},{"cell_type":"code","execution_count":43,"metadata":{"id":"Q8rMfqdQB6Ik","executionInfo":{"status":"ok","timestamp":1668182081532,"user_tz":180,"elapsed":1068,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["###########################################\n","### Just run this cell to load the data ###\n","###########################################\n","\n","#import tensorflow_datasets as tfds\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","def load_data(percentage_of_sentences=None):\n","    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n","\n","    train_sentences, y_train = tfds.as_numpy(train_data)\n","    test_sentences, y_test = tfds.as_numpy(test_data)\n","    \n","    # Take only a given percentage of the entire data\n","    if percentage_of_sentences is not None:\n","        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n","        \n","        len_train = int(percentage_of_sentences/100*len(train_sentences))\n","        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n","  \n","        len_test = int(percentage_of_sentences/100*len(test_sentences))\n","        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n","    \n","    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n","    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n","    \n","    return X_train, y_train, X_test, y_test\n","\n","X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"]},{"cell_type":"markdown","metadata":{"id":"ttDRlDFzB6Il"},"source":["Now that you have loaded the data, let's check it out!\n","\n","❓ **Question** ❓ You can play with the data here. In particular, `X_train` and `X_test` are lists of sentences. Let's print some of them, with their respective label stored in `y_train` and `y_test`."]},{"cell_type":"code","source":["len(X_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"geVm3nDuOf_j","executionInfo":{"status":"ok","timestamp":1668182082183,"user_tz":180,"elapsed":33,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"70254f16-5343-4c29-cca7-9faf3492008a"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2500"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vNhvLEbCWGW","executionInfo":{"status":"ok","timestamp":1668182082183,"user_tz":180,"elapsed":29,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"0f91b4f2-61a4-4eab-e0a0-4c3cb6904f36"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2500,)"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","execution_count":46,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"6nDqHNZKB6Il","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":27,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"831e4ee8-6a9d-4c88-b9c2-8fbb2e1b6598"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function list.index(value, start=0, stop=9223372036854775807, /)>"]},"metadata":{},"execution_count":46}],"source":["X_train.index"]},{"cell_type":"markdown","metadata":{"id":"a-LKzr4rB6Im"},"source":["**LABELS**, the task is a binary classification problem:\n","- label 0 corresponds to a negative movie review\n","- label 1 corresponds to a positive movie review\n","\n","**INPUTS**: The data has been partially cleaned! So you don't have to worry about it in this exercise. But don't forget this step in real-life challenges. \n","\n","Remember that words are not computer-compatible materials? You have to tokenize them!\n","\n","❓ **Question** ❓ Run the following cell to tokenize your sentences"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"V8tpGnvDB6Im","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":24,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# This initializes a Keras utilities that does all the tokenization for you\n","tokenizer = Tokenizer()\n","\n","# The tokenization learns a dictionary that maps a token (integer) to each word\n","# It can be done only on the train set - we are not supposed to know the test set!\n","# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n","tokenizer.fit_on_texts(X_train)\n","    \n","# We apply the tokenization to the train and test set\n","X_train_token = tokenizer.texts_to_sequences(X_train)\n","X_test_token = tokenizer.texts_to_sequences(X_test)"]},{"cell_type":"markdown","metadata":{"id":"WmoMtemWB6Im"},"source":["❓ **Question** ❓ Print some of the tokenized sentences to be sure you got what you expected"]},{"cell_type":"code","execution_count":48,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"egoWyJ-iB6Im","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":23,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"5ea22ee0-1453-4c03-a519-010f018fdca5"},"outputs":[{"output_type":"stream","name":"stdout","text":["['mann', 'photographs', 'the', 'alberta', 'rocky', 'mountains', 'in', 'a', 'superb', 'fashion', 'and', 'jimmy', 'stewart', 'and', 'walter', 'brennan', 'give', 'enjoyable', 'performances', 'as', 'they', 'always', 'seem', 'to', 'do', 'br', 'br', 'but', 'come', 'on', 'hollywood', 'a', 'mountie', 'telling', 'the', 'people', 'of', 'dawson', 'city', 'yukon', 'to', 'elect', 'themselves', 'a', 'marshal', 'yes', 'a', 'marshal', 'and', 'to', 'enforce', 'the', 'law', 'themselves', 'then', 'gunfighters', 'battling', 'it', 'out', 'on', 'the', 'streets', 'for', 'control', 'of', 'the', 'town', 'br', 'br', 'nothing', 'even', 'remotely', 'resembling', 'that', 'happened', 'on', 'the', 'canadian', 'side', 'of', 'the', 'border', 'during', 'the', 'klondike', 'gold', 'rush', 'mr', 'mann', 'and', 'company', 'appear', 'to', 'have', 'mistaken', 'dawson', 'city', 'for', 'deadwood', 'the', 'canadian', 'north', 'for', 'the', 'american', 'wild', 'west', 'br', 'br', 'canadian', 'viewers', 'be', 'prepared', 'for', 'a', 'reefer', 'madness', 'type', 'of', 'enjoyable', 'howl', 'with', 'this', 'ludicrous', 'plot', 'or', 'to', 'shake', 'your', 'head', 'in', 'disgust']\n"]}],"source":["print(X_train[2])"]},{"cell_type":"code","source":["tokenizer.word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xz3IwE9OCsHF","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":21,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"e13015bb-7465-48d6-dd6c-03d9c4c8782a"},"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'the': 1,\n"," 'a': 2,\n"," 'and': 3,\n"," 'of': 4,\n"," 'to': 5,\n"," 'is': 6,\n"," 'br': 7,\n"," 'in': 8,\n"," 'i': 9,\n"," 'it': 10,\n"," 'this': 11,\n"," 'that': 12,\n"," 'was': 13,\n"," 'as': 14,\n"," 'for': 15,\n"," 'with': 16,\n"," 'but': 17,\n"," 'movie': 18,\n"," 'film': 19,\n"," 'on': 20,\n"," 'not': 21,\n"," 'you': 22,\n"," 'his': 23,\n"," 'are': 24,\n"," 'have': 25,\n"," 'one': 26,\n"," 'be': 27,\n"," 'he': 28,\n"," 'all': 29,\n"," 'at': 30,\n"," 'by': 31,\n"," 'they': 32,\n"," 'an': 33,\n"," 'so': 34,\n"," 'like': 35,\n"," 'who': 36,\n"," 'from': 37,\n"," 'her': 38,\n"," 'or': 39,\n"," 'just': 40,\n"," 'if': 41,\n"," 'out': 42,\n"," 'about': 43,\n"," \"it's\": 44,\n"," 'has': 45,\n"," 'what': 46,\n"," 'some': 47,\n"," 'there': 48,\n"," 'good': 49,\n"," 'more': 50,\n"," 'when': 51,\n"," 'very': 52,\n"," 'no': 53,\n"," 'up': 54,\n"," 'she': 55,\n"," 'my': 56,\n"," 'time': 57,\n"," 'even': 58,\n"," 'which': 59,\n"," 'would': 60,\n"," 'really': 61,\n"," 'only': 62,\n"," 'had': 63,\n"," 'story': 64,\n"," 'me': 65,\n"," 'see': 66,\n"," 'can': 67,\n"," 'their': 68,\n"," 'were': 69,\n"," 'well': 70,\n"," 'than': 71,\n"," 'much': 72,\n"," 'get': 73,\n"," 'do': 74,\n"," 'great': 75,\n"," 'been': 76,\n"," 'we': 77,\n"," 'first': 78,\n"," 'bad': 79,\n"," 'because': 80,\n"," 'into': 81,\n"," 'other': 82,\n"," 'will': 83,\n"," 'how': 84,\n"," 'also': 85,\n"," 'most': 86,\n"," 'then': 87,\n"," 'too': 88,\n"," 'him': 89,\n"," 'made': 90,\n"," \"don't\": 91,\n"," 'people': 92,\n"," 'way': 93,\n"," 'them': 94,\n"," 'make': 95,\n"," 'its': 96,\n"," 'movies': 97,\n"," 'could': 98,\n"," 'any': 99,\n"," 'films': 100,\n"," 'think': 101,\n"," 'watch': 102,\n"," 'plot': 103,\n"," 'after': 104,\n"," 'two': 105,\n"," 'best': 106,\n"," 'many': 107,\n"," 'acting': 108,\n"," 'life': 109,\n"," 'character': 110,\n"," 'characters': 111,\n"," 'never': 112,\n"," 'seen': 113,\n"," 'where': 114,\n"," 'being': 115,\n"," 'did': 116,\n"," 'over': 117,\n"," 'little': 118,\n"," 'these': 119,\n"," 'love': 120,\n"," 'end': 121,\n"," 'man': 122,\n"," 'show': 123,\n"," 'better': 124,\n"," 'off': 125,\n"," 'ever': 126,\n"," 'know': 127,\n"," 'still': 128,\n"," 'does': 129,\n"," 'say': 130,\n"," 'while': 131,\n"," 'your': 132,\n"," 'scene': 133,\n"," 'go': 134,\n"," 'why': 135,\n"," 'should': 136,\n"," 'through': 137,\n"," 'such': 138,\n"," 'here': 139,\n"," 'scenes': 140,\n"," 'something': 141,\n"," 'real': 142,\n"," 'back': 143,\n"," 'now': 144,\n"," 'thing': 145,\n"," \"i'm\": 146,\n"," \"doesn't\": 147,\n"," 'find': 148,\n"," 'before': 149,\n"," \"didn't\": 150,\n"," 'though': 151,\n"," 'makes': 152,\n"," 'old': 153,\n"," 'again': 154,\n"," 'watching': 155,\n"," 'funny': 156,\n"," 'new': 157,\n"," 'those': 158,\n"," 'every': 159,\n"," 'director': 160,\n"," 'actors': 161,\n"," 'years': 162,\n"," 'going': 163,\n"," 'nothing': 164,\n"," 'actually': 165,\n"," 'another': 166,\n"," 'look': 167,\n"," 'same': 168,\n"," 'work': 169,\n"," 'part': 170,\n"," '10': 171,\n"," 'pretty': 172,\n"," 'want': 173,\n"," 'quite': 174,\n"," 'few': 175,\n"," 'young': 176,\n"," 'thought': 177,\n"," 'down': 178,\n"," 'bit': 179,\n"," 'horror': 180,\n"," 'own': 181,\n"," 'cast': 182,\n"," \"can't\": 183,\n"," 'give': 184,\n"," 'around': 185,\n"," 'fact': 186,\n"," 'lot': 187,\n"," \"that's\": 188,\n"," 'world': 189,\n"," 'things': 190,\n"," 'enough': 191,\n"," 'point': 192,\n"," 'series': 193,\n"," 'both': 194,\n"," 'may': 195,\n"," 'done': 196,\n"," 'long': 197,\n"," 'seems': 198,\n"," 'got': 199,\n"," 'however': 200,\n"," 'gets': 201,\n"," 'take': 202,\n"," 'role': 203,\n"," 'us': 204,\n"," 'always': 205,\n"," 'original': 206,\n"," 'between': 207,\n"," 'big': 208,\n"," 'right': 209,\n"," 'saw': 210,\n"," 'music': 211,\n"," 'times': 212,\n"," 'without': 213,\n"," \"i've\": 214,\n"," \"isn't\": 215,\n"," 'family': 216,\n"," 'far': 217,\n"," 'whole': 218,\n"," 'might': 219,\n"," 'minutes': 220,\n"," 'am': 221,\n"," 'come': 222,\n"," 'least': 223,\n"," \"there's\": 224,\n"," '2': 225,\n"," 'action': 226,\n"," 'script': 227,\n"," 'must': 228,\n"," 'tv': 229,\n"," 'since': 230,\n"," 'kind': 231,\n"," 'away': 232,\n"," 'interesting': 233,\n"," 'almost': 234,\n"," 'guy': 235,\n"," 'fun': 236,\n"," 'performance': 237,\n"," 'rather': 238,\n"," 'especially': 239,\n"," 'anything': 240,\n"," 'hard': 241,\n"," 'last': 242,\n"," 'actor': 243,\n"," 'sure': 244,\n"," 'believe': 245,\n"," 'three': 246,\n"," 'girl': 247,\n"," 'comedy': 248,\n"," \"he's\": 249,\n"," 'ending': 250,\n"," 'making': 251,\n"," 'worst': 252,\n"," 'looking': 253,\n"," 'anyone': 254,\n"," 'woman': 255,\n"," 'screen': 256,\n"," 'day': 257,\n"," 'trying': 258,\n"," 'maybe': 259,\n"," 'found': 260,\n"," 'course': 261,\n"," 'watched': 262,\n"," 'each': 263,\n"," 'having': 264,\n"," 'set': 265,\n"," 'probably': 266,\n"," 'yet': 267,\n"," 'although': 268,\n"," 'book': 269,\n"," 'comes': 270,\n"," 'feel': 271,\n"," 'money': 272,\n"," 'everything': 273,\n"," 'played': 274,\n"," \"wasn't\": 275,\n"," 'night': 276,\n"," 'true': 277,\n"," 'goes': 278,\n"," 'instead': 279,\n"," 'put': 280,\n"," 'sense': 281,\n"," 'reason': 282,\n"," 'place': 283,\n"," 'high': 284,\n"," 'dvd': 285,\n"," 'audience': 286,\n"," 'main': 287,\n"," 'looks': 288,\n"," 'later': 289,\n"," '1': 290,\n"," 'different': 291,\n"," 'job': 292,\n"," 'second': 293,\n"," 'once': 294,\n"," 'shows': 295,\n"," 'during': 296,\n"," 'american': 297,\n"," 'else': 298,\n"," 'together': 299,\n"," 'takes': 300,\n"," 'worth': 301,\n"," 'someone': 302,\n"," 'plays': 303,\n"," 'fan': 304,\n"," 'wife': 305,\n"," 'our': 306,\n"," 'said': 307,\n"," 'play': 308,\n"," 'star': 309,\n"," 'year': 310,\n"," 'read': 311,\n"," 'effects': 312,\n"," 'version': 313,\n"," 'home': 314,\n"," 'special': 315,\n"," 'half': 316,\n"," 'black': 317,\n"," 'idea': 318,\n"," 'boring': 319,\n"," 'seeing': 320,\n"," 'given': 321,\n"," 'help': 322,\n"," '3': 323,\n"," 'less': 324,\n"," 'war': 325,\n"," 'beautiful': 326,\n"," 'house': 327,\n"," 'seem': 328,\n"," 'himself': 329,\n"," 'death': 330,\n"," \"you're\": 331,\n"," 'production': 332,\n"," 'left': 333,\n"," 'mind': 334,\n"," 'excellent': 335,\n"," 'simply': 336,\n"," 'john': 337,\n"," 'top': 338,\n"," 'either': 339,\n"," 'short': 340,\n"," 'camera': 341,\n"," 'hollywood': 342,\n"," 'stupid': 343,\n"," 'poor': 344,\n"," 'shot': 345,\n"," 'couple': 346,\n"," 'father': 347,\n"," 'used': 348,\n"," 'perfect': 349,\n"," 'line': 350,\n"," 'dead': 351,\n"," 'until': 352,\n"," 'nice': 353,\n"," \"couldn't\": 354,\n"," 'kids': 355,\n"," 'understand': 356,\n"," 'try': 357,\n"," 'enjoy': 358,\n"," 'classic': 359,\n"," 'completely': 360,\n"," 'mean': 361,\n"," 'along': 362,\n"," 'wrong': 363,\n"," 'awful': 364,\n"," 'rest': 365,\n"," 'perhaps': 366,\n"," 'everyone': 367,\n"," 'start': 368,\n"," 'need': 369,\n"," 'performances': 370,\n"," 'truly': 371,\n"," 'men': 372,\n"," 'boy': 373,\n"," \"i'd\": 374,\n"," 'name': 375,\n"," 'finally': 376,\n"," 'liked': 377,\n"," 'budget': 378,\n"," 'keep': 379,\n"," 'small': 380,\n"," 'gives': 381,\n"," 'head': 382,\n"," 'felt': 383,\n"," 'wonderful': 384,\n"," 'full': 385,\n"," 'face': 386,\n"," 'style': 387,\n"," '\\x96': 388,\n"," 'early': 389,\n"," 'mr': 390,\n"," 'stars': 391,\n"," 'let': 392,\n"," 'low': 393,\n"," 'friends': 394,\n"," 'tell': 395,\n"," 'drama': 396,\n"," 'hope': 397,\n"," 'written': 398,\n"," 'school': 399,\n"," 'dialogue': 400,\n"," 'recommend': 401,\n"," 'terrible': 402,\n"," 'case': 403,\n"," 'worse': 404,\n"," 'based': 405,\n"," 'sort': 406,\n"," 'moments': 407,\n"," 'run': 408,\n"," 'playing': 409,\n"," 'next': 410,\n"," 'video': 411,\n"," 'supposed': 412,\n"," 'mother': 413,\n"," 'often': 414,\n"," 'getting': 415,\n"," 'yes': 416,\n"," 'become': 417,\n"," 'use': 418,\n"," 'loved': 419,\n"," 'human': 420,\n"," 'came': 421,\n"," 'absolutely': 422,\n"," 'others': 423,\n"," 'totally': 424,\n"," 'called': 425,\n"," 'lines': 426,\n"," 'fans': 427,\n"," 'certainly': 428,\n"," 'side': 429,\n"," \"they're\": 430,\n"," 'days': 431,\n"," 'doing': 432,\n"," 'entertaining': 433,\n"," 'example': 434,\n"," 'dark': 435,\n"," 'overall': 436,\n"," 'live': 437,\n"," 'art': 438,\n"," 'killer': 439,\n"," 'definitely': 440,\n"," 'lost': 441,\n"," 'went': 442,\n"," 'seemed': 443,\n"," 'city': 444,\n"," 'beginning': 445,\n"," 'friend': 446,\n"," 'tries': 447,\n"," 'problem': 448,\n"," 'town': 449,\n"," 'picture': 450,\n"," 'turn': 451,\n"," 'god': 452,\n"," 'remember': 453,\n"," 'against': 454,\n"," 'evil': 455,\n"," 'entire': 456,\n"," 'care': 457,\n"," 'wanted': 458,\n"," 'already': 459,\n"," 'fine': 460,\n"," 'sex': 461,\n"," 'actress': 462,\n"," 'close': 463,\n"," 'women': 464,\n"," 'itself': 465,\n"," 'particularly': 466,\n"," 'title': 467,\n"," 'history': 468,\n"," 'act': 469,\n"," 'throughout': 470,\n"," 'writing': 471,\n"," 'myself': 472,\n"," 'episode': 473,\n"," \"won't\": 474,\n"," 'white': 475,\n"," 'under': 476,\n"," 'cinema': 477,\n"," 'etc': 478,\n"," 'sound': 479,\n"," \"she's\": 480,\n"," 'unfortunately': 481,\n"," 'anyway': 482,\n"," 'oh': 483,\n"," 'wants': 484,\n"," 'hand': 485,\n"," 'direction': 486,\n"," 'piece': 487,\n"," 'cannot': 488,\n"," 'daughter': 489,\n"," 'stuff': 490,\n"," '5': 491,\n"," 'behind': 492,\n"," 'despite': 493,\n"," 'themselves': 494,\n"," 'humor': 495,\n"," 'lives': 496,\n"," 'children': 497,\n"," 'eyes': 498,\n"," 'lead': 499,\n"," 'highly': 500,\n"," 'able': 501,\n"," 'late': 502,\n"," 'moment': 503,\n"," 'hour': 504,\n"," 'person': 505,\n"," 'becomes': 506,\n"," 'kid': 507,\n"," 'waste': 508,\n"," 'child': 509,\n"," 'parts': 510,\n"," 'leave': 511,\n"," 'favorite': 512,\n"," 'viewer': 513,\n"," 'kill': 514,\n"," 'slow': 515,\n"," 'says': 516,\n"," 'thinking': 517,\n"," \"you'll\": 518,\n"," 'guys': 519,\n"," 'murder': 520,\n"," 'soon': 521,\n"," 'decent': 522,\n"," 'age': 523,\n"," 'past': 524,\n"," 'expect': 525,\n"," 'girls': 526,\n"," 'type': 527,\n"," 'guess': 528,\n"," 'voice': 529,\n"," '4': 530,\n"," 'happen': 531,\n"," 'brilliant': 532,\n"," 'b': 533,\n"," 'obvious': 534,\n"," 'matter': 535,\n"," 'several': 536,\n"," 'starts': 537,\n"," 'feeling': 538,\n"," 'michael': 539,\n"," 'heart': 540,\n"," 'final': 541,\n"," 'genre': 542,\n"," 'ok': 543,\n"," 'looked': 544,\n"," 'directed': 545,\n"," 'except': 546,\n"," 'involved': 547,\n"," 'flick': 548,\n"," 'police': 549,\n"," 'chance': 550,\n"," 'sometimes': 551,\n"," 'laugh': 552,\n"," 'blood': 553,\n"," 'opinion': 554,\n"," 'including': 555,\n"," 'experience': 556,\n"," 'number': 557,\n"," 'strong': 558,\n"," 'alone': 559,\n"," 'power': 560,\n"," 'gave': 561,\n"," 'robert': 562,\n"," 'attempt': 563,\n"," 'score': 564,\n"," 'works': 565,\n"," 'possible': 566,\n"," 'writer': 567,\n"," 'david': 568,\n"," 'told': 569,\n"," 'annoying': 570,\n"," 'somewhat': 571,\n"," 'known': 572,\n"," \"i'll\": 573,\n"," 'wonder': 574,\n"," 'stop': 575,\n"," 'cut': 576,\n"," \"aren't\": 577,\n"," 'episodes': 578,\n"," 'enjoyed': 579,\n"," 'killed': 580,\n"," 'took': 581,\n"," 'turned': 582,\n"," 'wish': 583,\n"," 'happens': 584,\n"," 'words': 585,\n"," 'turns': 586,\n"," \"film's\": 587,\n"," 'amazing': 588,\n"," 'english': 589,\n"," 'happened': 590,\n"," 'today': 591,\n"," 'lady': 592,\n"," 'view': 593,\n"," 'whose': 594,\n"," 'sequel': 595,\n"," 'horrible': 596,\n"," 'hit': 597,\n"," 'ago': 598,\n"," 'jane': 599,\n"," 'simple': 600,\n"," 'female': 601,\n"," 'quality': 602,\n"," 'scary': 603,\n"," 'coming': 604,\n"," 'seriously': 605,\n"," 'mystery': 606,\n"," 'save': 607,\n"," 'james': 608,\n"," 'relationship': 609,\n"," 'group': 610,\n"," 'please': 611,\n"," 'roles': 612,\n"," 'novel': 613,\n"," 'single': 614,\n"," 's': 615,\n"," 'middle': 616,\n"," 'musical': 617,\n"," 'hell': 618,\n"," 'across': 619,\n"," 'word': 620,\n"," \"wouldn't\": 621,\n"," 'level': 622,\n"," 'serious': 623,\n"," 'lack': 624,\n"," 'usually': 625,\n"," 'mostly': 626,\n"," 'shown': 627,\n"," 'eye': 628,\n"," 'king': 629,\n"," 'red': 630,\n"," 'sad': 631,\n"," 'crap': 632,\n"," 'bring': 633,\n"," 'york': 634,\n"," 'body': 635,\n"," 'running': 636,\n"," 'car': 637,\n"," 'jokes': 638,\n"," 'herself': 639,\n"," 'hours': 640,\n"," 'light': 641,\n"," 'self': 642,\n"," 'happy': 643,\n"," '7': 644,\n"," 'gore': 645,\n"," 'events': 646,\n"," 'obviously': 647,\n"," \"'\": 648,\n"," 'room': 649,\n"," 'basically': 650,\n"," 'british': 651,\n"," 'fight': 652,\n"," 'important': 653,\n"," 'miss': 654,\n"," 'comic': 655,\n"," 'cool': 656,\n"," 'son': 657,\n"," 'husband': 658,\n"," 'interest': 659,\n"," 'extremely': 660,\n"," 'documentary': 661,\n"," 'due': 662,\n"," 'local': 663,\n"," 'stories': 664,\n"," 'hilarious': 665,\n"," 'none': 666,\n"," 'predictable': 667,\n"," 'shots': 668,\n"," 'thriller': 669,\n"," 'started': 670,\n"," 'huge': 671,\n"," 'brother': 672,\n"," 'straight': 673,\n"," 'elements': 674,\n"," 'taken': 675,\n"," 'ten': 676,\n"," 'lame': 677,\n"," 'call': 678,\n"," 'dull': 679,\n"," 'oscar': 680,\n"," 'above': 681,\n"," 'yourself': 682,\n"," 'add': 683,\n"," 'richard': 684,\n"," 'near': 685,\n"," 'surprised': 686,\n"," 'rating': 687,\n"," 'released': 688,\n"," 'game': 689,\n"," 'clearly': 690,\n"," 'hero': 691,\n"," 'sets': 692,\n"," 'usual': 693,\n"," 'ridiculous': 694,\n"," 'storyline': 695,\n"," 'disappointed': 696,\n"," 'complete': 697,\n"," 'talent': 698,\n"," 'romantic': 699,\n"," 'entertainment': 700,\n"," 'country': 701,\n"," 'television': 702,\n"," \"let's\": 703,\n"," 'lee': 704,\n"," 'fast': 705,\n"," 'supporting': 706,\n"," 'violence': 707,\n"," 'similar': 708,\n"," 'dialog': 709,\n"," '9': 710,\n"," 'falls': 711,\n"," 'heard': 712,\n"," 'apparently': 713,\n"," 'finds': 714,\n"," 'japanese': 715,\n"," 'nor': 716,\n"," 'giving': 717,\n"," 'twist': 718,\n"," 'beyond': 719,\n"," 'attention': 720,\n"," 'ones': 721,\n"," 'whether': 722,\n"," 'editing': 723,\n"," 'songs': 724,\n"," 'tale': 725,\n"," 'song': 726,\n"," 'die': 727,\n"," \"haven't\": 728,\n"," 'minute': 729,\n"," 'fall': 730,\n"," 'peter': 731,\n"," 'lots': 732,\n"," 'exactly': 733,\n"," 'figure': 734,\n"," 'saying': 735,\n"," 'taking': 736,\n"," 'moving': 737,\n"," 'possibly': 738,\n"," 'reality': 739,\n"," 'screenplay': 740,\n"," 'strange': 741,\n"," 'wait': 742,\n"," 'non': 743,\n"," 'jack': 744,\n"," 'typical': 745,\n"," 'comments': 746,\n"," 'stay': 747,\n"," 'means': 748,\n"," 'future': 749,\n"," 'realistic': 750,\n"," 'cinematography': 751,\n"," 'mention': 752,\n"," 'sit': 753,\n"," 'somehow': 754,\n"," 'sorry': 755,\n"," 'named': 756,\n"," 'animation': 757,\n"," 'indeed': 758,\n"," 'message': 759,\n"," 'talk': 760,\n"," 'living': 761,\n"," 'street': 762,\n"," 'modern': 763,\n"," 'rock': 764,\n"," 'ends': 765,\n"," \"who's\": 766,\n"," 'class': 767,\n"," 'earth': 768,\n"," 'kept': 769,\n"," 'brought': 770,\n"," 'imagine': 771,\n"," 'major': 772,\n"," 'third': 773,\n"," 'nearly': 774,\n"," 'sequence': 775,\n"," 'atmosphere': 776,\n"," 'bunch': 777,\n"," 'theme': 778,\n"," 'reviews': 779,\n"," 'christmas': 780,\n"," 'whom': 781,\n"," 'expected': 782,\n"," 'fantastic': 783,\n"," 'hate': 784,\n"," 'opening': 785,\n"," 'easily': 786,\n"," 'check': 787,\n"," 'imdb': 788,\n"," 'doubt': 789,\n"," 'enjoyable': 790,\n"," 'tells': 791,\n"," 'baby': 792,\n"," 'appears': 793,\n"," 'sounds': 794,\n"," 'career': 795,\n"," 'laughs': 796,\n"," 'writers': 797,\n"," 'air': 798,\n"," 'nature': 799,\n"," 're': 800,\n"," 'cheap': 801,\n"," 'dream': 802,\n"," 'sadly': 803,\n"," 'easy': 804,\n"," 'emotional': 805,\n"," 'within': 806,\n"," 'talking': 807,\n"," 't': 808,\n"," 'ways': 809,\n"," 'knew': 810,\n"," 'begins': 811,\n"," 'four': 812,\n"," 'five': 813,\n"," 'effort': 814,\n"," 'viewers': 815,\n"," 'silly': 816,\n"," 'material': 817,\n"," 'comment': 818,\n"," 'knows': 819,\n"," 'meet': 820,\n"," 'stand': 821,\n"," 'parents': 822,\n"," 'difficult': 823,\n"," 'hot': 824,\n"," 'upon': 825,\n"," 'leads': 826,\n"," 'avoid': 827,\n"," 'points': 828,\n"," 'suspense': 829,\n"," 'french': 830,\n"," 'gone': 831,\n"," 'believable': 832,\n"," 'weird': 833,\n"," 'wasted': 834,\n"," 'review': 835,\n"," 'period': 836,\n"," 'dramatic': 837,\n"," 'season': 838,\n"," 'leading': 839,\n"," 'wrote': 840,\n"," 'books': 841,\n"," 'george': 842,\n"," 'order': 843,\n"," 'directors': 844,\n"," \"what's\": 845,\n"," 'directing': 846,\n"," 'hear': 847,\n"," 'portrayed': 848,\n"," 'famous': 849,\n"," 'using': 850,\n"," 'casting': 851,\n"," '8': 852,\n"," 'release': 853,\n"," 'working': 854,\n"," 'change': 855,\n"," 'problems': 856,\n"," 'among': 857,\n"," 'feels': 858,\n"," 'theater': 859,\n"," 'tried': 860,\n"," 'appear': 861,\n"," 'setting': 862,\n"," 'needs': 863,\n"," 'married': 864,\n"," 'deal': 865,\n"," 'sequences': 866,\n"," 'decided': 867,\n"," 'perfectly': 868,\n"," 'begin': 869,\n"," 'hair': 870,\n"," 'clear': 871,\n"," 'interested': 872,\n"," \"you've\": 873,\n"," 'space': 874,\n"," 'return': 875,\n"," 'poorly': 876,\n"," 'depth': 877,\n"," 'western': 878,\n"," 'box': 879,\n"," 'odd': 880,\n"," 'follow': 881,\n"," 'social': 882,\n"," 'girlfriend': 883,\n"," 'meant': 884,\n"," 'particular': 885,\n"," 'premise': 886,\n"," 'otherwise': 887,\n"," 'forced': 888,\n"," 'shame': 889,\n"," 'potential': 890,\n"," 'de': 891,\n"," 'worked': 892,\n"," 'eventually': 893,\n"," 'meets': 894,\n"," 'sister': 895,\n"," 'stage': 896,\n"," 'fantasy': 897,\n"," 'animals': 898,\n"," 'became': 899,\n"," 'previous': 900,\n"," 'needed': 901,\n"," 'whatever': 902,\n"," 'earlier': 903,\n"," 'stewart': 904,\n"," 'waiting': 905,\n"," 'brings': 906,\n"," 'cute': 907,\n"," 'agree': 908,\n"," 'failed': 909,\n"," 'showing': 910,\n"," 'result': 911,\n"," 'effect': 912,\n"," 'bought': 913,\n"," 'gay': 914,\n"," 'badly': 915,\n"," 'note': 916,\n"," 'creepy': 917,\n"," 'fire': 918,\n"," 'paul': 919,\n"," 'apart': 920,\n"," 'move': 921,\n"," 'mary': 922,\n"," 'general': 923,\n"," 'form': 924,\n"," 'average': 925,\n"," 'doctor': 926,\n"," 'co': 927,\n"," 'pathetic': 928,\n"," 'admit': 929,\n"," 'sci': 930,\n"," 'fi': 931,\n"," 'feature': 932,\n"," 'frank': 933,\n"," 'honestly': 934,\n"," 'rent': 935,\n"," 'ask': 936,\n"," 'greatest': 937,\n"," 'band': 938,\n"," 'unless': 939,\n"," 'crime': 940,\n"," 'bored': 941,\n"," 'society': 942,\n"," 'hands': 943,\n"," 'beauty': 944,\n"," 'basic': 945,\n"," 'kelly': 946,\n"," 'fighting': 947,\n"," 'features': 948,\n"," 'dance': 949,\n"," \"we're\": 950,\n"," 'filmed': 951,\n"," 'hardly': 952,\n"," '20': 953,\n"," 'romance': 954,\n"," 'members': 955,\n"," 'cartoon': 956,\n"," 'older': 957,\n"," 'male': 958,\n"," 'weak': 959,\n"," 'copy': 960,\n"," 'ben': 961,\n"," 'free': 962,\n"," 'cop': 963,\n"," 'science': 964,\n"," 'certain': 965,\n"," 'monster': 966,\n"," 'write': 967,\n"," 'buy': 968,\n"," 'expecting': 969,\n"," 'situation': 970,\n"," 'surprise': 971,\n"," 'cat': 972,\n"," 'team': 973,\n"," 'consider': 974,\n"," 'state': 975,\n"," 'soundtrack': 976,\n"," 'credits': 977,\n"," 'actual': 978,\n"," 'footage': 979,\n"," 'break': 980,\n"," 'trash': 981,\n"," 'joe': 982,\n"," 'tom': 983,\n"," 'cheesy': 984,\n"," 'crazy': 985,\n"," 'front': 986,\n"," 'control': 987,\n"," 'business': 988,\n"," 'deserves': 989,\n"," 'decide': 990,\n"," 'sexual': 991,\n"," 'dumb': 992,\n"," 'present': 993,\n"," 'forward': 994,\n"," 'zombie': 995,\n"," 'sees': 996,\n"," 'joke': 997,\n"," 'attempts': 998,\n"," 'recently': 999,\n"," '15': 1000,\n"," ...}"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"Eh9wkpMYB6Im"},"source":["The dictionary that maps each word to a token can be accessed with `tokenizer.word_index`\n","    \n","❓ **Question** ❓ Add a `vocab_size` variable that stores the number of different words (=tokens) in the train set. This is called the _size of the vocabulary_."]},{"cell_type":"code","execution_count":50,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"bp1TXn3NB6In","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":18,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"85c6a83c-2121-4df0-a50d-d48bc5e8e466"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["30419"]},"metadata":{},"execution_count":50}],"source":["vocab_size = len(tokenizer.word_index)\n","vocab_size"]},{"cell_type":"code","source":["len(X_test_token[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cr0BfdhgNCRg","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"51c7fea7-ee9a-44c1-9350-141a6724ae5f"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["277"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["len(X_train_token[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMWVyYmLOWhI","executionInfo":{"status":"ok","timestamp":1668182082184,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"08ac24dc-fede-46c1-98d5-779931ed6f4f"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["112"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"0mglmNSGB6In"},"source":["Your `X_train_token` and `X_test_token` contain sequences of different lengths.\n","\n","<img src=\"padding.png\" alt='Word2Vec' width=\"700px\" />\n","\n","However, a neural network has to have a tensor as input. For this reason, you have to pad your data.\n","\n","❓ **Question** ❓  Pad your data with the `pad_sequences` function (documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)). Do not forget about the `dtype` and `padding` keywords (but do not use `maxlen` here)."]},{"cell_type":"code","execution_count":53,"metadata":{"tags":["challengify"],"id":"lDZCKi7sB6In","executionInfo":{"status":"ok","timestamp":1668182082185,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from keras_preprocessing.sequence import pad_sequences\n","#from tensorflow.keras.preprocessing.sequence import pad_sequences\n","X_train_token_pad = pad_sequences(X_train_token, dtype='float32',padding = 'post',value = 0) "]},{"cell_type":"code","source":["X_train_token_pad.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qtW74ijUNlze","executionInfo":{"status":"ok","timestamp":1668182082185,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"170cb7a9-4995-4d5d-c463-c0fc48323de2"},"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2500, 1164)"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"NkS99-uBB6In"},"source":["# The RNN\n","\n","Let's now feed this data to a Recurrent Neural Network.\n","\n","❓ **Question** ❓ Write a model that has:\n","- an embedding layer whose `input_dim` is the size of your vocabulary (= your `vocab_size`), and whose `output_dim` is the size of the embedding space you want to have\n","- a RNN (SimpleRNN, LSTM, GRU) layer\n","- a Dense layer\n","- an output layer\n","\n","⚠️ **Warning** ⚠️ Here, you don't need a masking layer. Why? Because `layers.Embedding` has a argument to do that directly, which you have to set with `mask_zero=True`. That also means that your data **HAS TO** be padded with **0** (which is the default behavior). See the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding#example_2) to understand how it **impacts** the `input_dim`.\n","\n","<details>\n","    <summary>💡 Hint</summary>\n","\n","`input_dim` should equal size of vocabulary + 1\n","\n","</details>\n","\n","Compile it with the appropriate arguments"]},{"cell_type":"code","source":["from tensorflow.keras import layers, Sequential\n","from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n","\n","embedding_size = 100\n","model = Sequential()\n","model.add(layers.Embedding(input_dim=vocab_size+1, input_length=X_train_token_pad.shape[1], output_dim=embedding_size,mask_zero=True))\n","model.add(layers.LSTM(20))\n","model.add(layers.Dense(50, activation=\"relu\"))\n","model.add(layers.Dense(1, activation=\"sigmoid\"))\n","\n"],"metadata":{"id":"AwNn7pF6QTvH","executionInfo":{"status":"ok","timestamp":1668182117557,"user_tz":180,"elapsed":984,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zN20ahHZB6In"},"source":["❓ **Question** ❓ Look at the number of parameters in your RNN. "]},{"cell_type":"code","execution_count":57,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"1MTkMoBPB6In","executionInfo":{"status":"ok","timestamp":1668182121113,"user_tz":180,"elapsed":462,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"7ee9e701-f694-41bb-9ed6-0fa2837606df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_6 (Embedding)     (None, 1164, 100)         3042000   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 20)                9680      \n","                                                                 \n"," dense_3 (Dense)             (None, 50)                1050      \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 51        \n","                                                                 \n","=================================================================\n","Total params: 3,052,781\n","Trainable params: 3,052,781\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()\n"]},{"cell_type":"markdown","metadata":{"id":"nimSPmspB6Io"},"source":["❓ Double-check that the number of parameters in your embedding layer is equal to the (number of words in your vocabulary + 1 for the masking value) $\\times$  the dimension of your embedding."]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["challengify"],"id":"FG3pSJsaB6Io","executionInfo":{"status":"aborted","timestamp":1668182092495,"user_tz":180,"elapsed":23,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GFanMlqkB6Io"},"source":["❓ **Question** ❓ Start fitting your model with 20 epochs, with an early stopping criterion whose patience is equal to 4.\n","\n","⚠️ **Warning** ⚠️ You might see that it takes a lot of time! \n","\n","**So stop it after a couple of iterations!**"]},{"cell_type":"code","execution_count":58,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"eumMgzH2B6Io","executionInfo":{"status":"ok","timestamp":1668182147128,"user_tz":180,"elapsed":16841,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"6c22ad84-8b90-4700-c480-1e98d205893d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","16/16 [==============================] - 11s 159ms/step - loss: 0.6890 - accuracy: 0.5680 - val_loss: 0.6734 - val_accuracy: 0.6840\n","Epoch 2/20\n","16/16 [==============================] - 1s 42ms/step - loss: 0.5888 - accuracy: 0.7945 - val_loss: 0.5126 - val_accuracy: 0.8080\n","Epoch 3/20\n","16/16 [==============================] - 1s 41ms/step - loss: 0.3537 - accuracy: 0.8840 - val_loss: 0.5339 - val_accuracy: 0.7320\n","Epoch 4/20\n","16/16 [==============================] - 1s 44ms/step - loss: 0.1946 - accuracy: 0.9470 - val_loss: 0.4048 - val_accuracy: 0.8100\n","Epoch 5/20\n","16/16 [==============================] - 1s 46ms/step - loss: 0.0956 - accuracy: 0.9785 - val_loss: 0.4008 - val_accuracy: 0.8460\n","Epoch 6/20\n","16/16 [==============================] - 1s 41ms/step - loss: 0.0420 - accuracy: 0.9910 - val_loss: 0.4524 - val_accuracy: 0.8440\n","Epoch 7/20\n","16/16 [==============================] - 1s 42ms/step - loss: 0.0333 - accuracy: 0.9920 - val_loss: 0.5194 - val_accuracy: 0.8260\n","Epoch 8/20\n","16/16 [==============================] - 1s 46ms/step - loss: 0.0582 - accuracy: 0.9805 - val_loss: 0.4921 - val_accuracy: 0.7900\n","Epoch 9/20\n","16/16 [==============================] - 1s 44ms/step - loss: 0.0153 - accuracy: 0.9990 - val_loss: 0.4877 - val_accuracy: 0.8320\n"]}],"source":["model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","es = EarlyStopping(patience=4,restore_best_weights=True)\n","history = model.fit(X_train_token_pad, y_train, validation_split = 0.2 ,epochs = 20 ,batch_size = 128, verbose = 1, callbacks = [es])"]},{"cell_type":"markdown","metadata":{"id":"hdFsCzBIB6Io"},"source":["Let's not waste too much time just staring at our screen or having coffees. It is too early to start having breaks ;)\n","\n","❓ **Question** ❓ We will reduce the computational time. To start, let's first look at how many words there are in the different sentences of your train set (Just run the following cell)."]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"id":"2FfhAuiLB6Io","executionInfo":{"status":"ok","timestamp":1668182219789,"user_tz":180,"elapsed":491,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"10724797-4334-49f1-a2bb-84bb9b74de8a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","def plot_hist(X):\n","    len_ = [len(_) for _ in X]\n","    plt.hist(len_)\n","    plt.title('Histogram of the number of sentences that have a given number of words')\n","    plt.show()\n","    \n","plot_hist(X_train)\n"]},{"cell_type":"markdown","metadata":{"id":"WX0jif06B6Io"},"source":["You will probably see that 90 to 95% of your sentences have less than 300 words. And very few have more than 1000.\n","\n","However, as you didn't use `maxlen` in your padding above, your input tensor has a dimension equal to the length of the sentence that has the maximum number of words.\n","\n","Now, let's look at how this affects the padding: \n","\n","\n","<img src=\"tensor_size.png\" alt='Word2Vec' width=\"700px\" />\n","\n","Because of a few of very long sentences, one dimension of your tensor is equal to around 1000. However, most of the sentences with ~200 words have just padded values that are useless.\n","\n","So your tensor is mostly useless information, which still adds time to the training process.\n","\n","But what if you pad the data to a maximum length (`maxlen`) of say 200 (words)?\n","- First, that would increase the convergence and you would not need to stare at your screen while waiting for the algorithm to converge\n","- But in essence, do you really lose that much information? Do you think that you often need more than 200 words (up to 1000) to tell whether or not a sentence is positive of negative?\n","\n","❓ **Question** ❓ For all these reasons, re-do your padding using the `maxlen` keyword and retrain the model!  See how much faster it is now - without hurting the performance ;)"]},{"cell_type":"code","execution_count":63,"metadata":{"tags":["challengify"],"id":"jqc7FBEVB6Io","executionInfo":{"status":"ok","timestamp":1668182447987,"user_tz":180,"elapsed":521,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from keras_preprocessing.sequence import pad_sequences\n","#from tensorflow.keras.preprocessing.sequence import pad_sequences\n","X_train_token_pad = pad_sequences(X_train_token, dtype='float32',padding = 'post',value = 0,maxlen=200) "]},{"cell_type":"code","execution_count":64,"metadata":{"id":"PXqQX5DIB6Io","executionInfo":{"status":"ok","timestamp":1668182451549,"user_tz":180,"elapsed":606,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from tensorflow.keras import layers, Sequential\n","from tensorflow.keras.layers import SimpleRNN, LSTM, GRU\n","\n","embedding_size = 100\n","model = Sequential()\n","model.add(layers.Embedding(input_dim=vocab_size+1, input_length=X_train_token_pad.shape[1], output_dim=embedding_size,mask_zero=True))\n","model.add(layers.LSTM(20))\n","model.add(layers.Dense(50, activation=\"relu\"))\n","model.add(layers.Dense(1, activation=\"sigmoid\"))\n"]},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","es = EarlyStopping(patience=4,restore_best_weights=True)\n","history = model.fit(X_train_token_pad, y_train, validation_split = 0.2 ,epochs = 20 ,batch_size = 128, verbose = 1, callbacks = [es])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gid5J3YDa4XE","executionInfo":{"status":"ok","timestamp":1668182460830,"user_tz":180,"elapsed":7900,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"bbe8fe9c-f7a9-4d3b-ab99-df8dc6b3ac85"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","16/16 [==============================] - 6s 107ms/step - loss: 0.6876 - accuracy: 0.6070 - val_loss: 0.6683 - val_accuracy: 0.6780\n","Epoch 2/20\n","16/16 [==============================] - 0s 22ms/step - loss: 0.5548 - accuracy: 0.7980 - val_loss: 0.5319 - val_accuracy: 0.7180\n","Epoch 3/20\n","16/16 [==============================] - 0s 18ms/step - loss: 0.3321 - accuracy: 0.8845 - val_loss: 0.3986 - val_accuracy: 0.8360\n","Epoch 4/20\n","16/16 [==============================] - 0s 17ms/step - loss: 0.1791 - accuracy: 0.9540 - val_loss: 0.3812 - val_accuracy: 0.8360\n","Epoch 5/20\n","16/16 [==============================] - 0s 16ms/step - loss: 0.1009 - accuracy: 0.9760 - val_loss: 0.4506 - val_accuracy: 0.8380\n","Epoch 6/20\n","16/16 [==============================] - 0s 17ms/step - loss: 0.0545 - accuracy: 0.9875 - val_loss: 0.4205 - val_accuracy: 0.8300\n","Epoch 7/20\n","16/16 [==============================] - 0s 20ms/step - loss: 0.0200 - accuracy: 0.9970 - val_loss: 1.0232 - val_accuracy: 0.7360\n","Epoch 8/20\n","16/16 [==============================] - 0s 17ms/step - loss: 0.0308 - accuracy: 0.9935 - val_loss: 0.5654 - val_accuracy: 0.8240\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[{"file_id":"1HWhj3NkF4zW5crcd_NSyQ8nDsf0POMVR","timestamp":1668182483191}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}